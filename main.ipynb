{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: SQL\n",
    "\n",
    "There are two tables in the included database\n",
    "\n",
    "1. train_table\n",
    "2. test_table\n",
    "\n",
    "\n",
    "IMPORTANT: No other libraries are allowed to solve this tests only SQL Queries allowed\n",
    "\n",
    "* pandas methods are not is not allowed\n",
    "* sqlalchemy is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only library allowed do not modify\n",
    "import pandas as pd\n",
    "from src.sql import execute_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example query inside docstring\n",
    "execute_query(\"\"\"\n",
    "        SELECT * \n",
    "        FROM train_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example query inside docstring\n",
    "execute_query(\"\"\"\n",
    "        SELECT * \n",
    "        FROM test_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 1: Basic SQL\n",
    "Write a SQL statement from table <i>train_table</i> to obtain the top 10 <i>locations</i> people twit from (in descending order)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXAMPLE SOLUTION:\n",
    "\n",
    "   UserName          Location  count\n",
    "0     11756       Doha, Qatar     10\n",
    "..\n",
    "8       450    The Colony, TX      9\n",
    "9     14693  Brighton Lass ??      8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write query inside docstring\n",
    "query_1 = \"\"\"\n",
    "\n",
    "select \n",
    "*\n",
    "from (\n",
    "\n",
    "        SELECT \n",
    "               t.location              location, \n",
    "               count(t.OriginalTweet)  cantidad_twits -- para este caso se usó la columna OriginalTweet para dar visivilidad de \n",
    "                                                      -- que hay locaciones que están vacías \"None\" y de igual forma tienen tweets. \n",
    "                                                      -- otro punto que comentar es que encontré un tweet en blanco y no se si se deba sumarizar.\n",
    "                                                 \n",
    "        FROM train_table t \n",
    "        group by t.location \n",
    ")R\n",
    "order by r.cantidad_twits desc\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_1 = execute_query(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>cantidad_twits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>8590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>London</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London, England</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New York, NY</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>India</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UK</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          location  cantidad_twits\n",
       "0             None            8590\n",
       "1           London             540\n",
       "2    United States             528\n",
       "3  London, England             520\n",
       "4     New York, NY             395\n",
       "5   Washington, DC             373\n",
       "6   United Kingdom             337\n",
       "7  Los Angeles, CA             281\n",
       "8            India             268\n",
       "9               UK             232"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Intermediate SQL\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You are tasked to build a dataset that contains the count of Sentiment values as columns for every UserName.\n",
    "\n",
    "The dataset must contain the following columns and name convention:\n",
    "    0. UserName (int)\n",
    "    1. pos_count (int)\n",
    "    2. neg_count (int)\n",
    "    3. extremely_pos_count (int)\n",
    "    4. extremely_neg_count (int)\n",
    "\n",
    "EXAMPLE SOLUTION:\n",
    "\n",
    "\n",
    "   UserName  pos_count  neg_count  extremely_pos_count  extremely_neg_count\n",
    "0     14675          1          7                    1                    0\n",
    "1     12457          1          5                    0                    0\n",
    "...\n",
    "9     14405          0          4                    0                    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write query inside docstring\n",
    "query_2 = \"\"\"\n",
    "SELECT \n",
    "R.UserName,\n",
    "SUM(R.pos_count) pos_count,\n",
    "SUM(R.neg_count) neg_count,\n",
    "SUM(R.extremely_pos_count) extremely_pos_count,\n",
    "SUM(R.extremely_neg_count) extremely_neg_count\n",
    "\n",
    "FROM (\n",
    "     select                                                                t.UserName,\n",
    "            CASE WHEN t.sentiment = 'Positive'           THEN 1 else 0 end pos_count,\n",
    "            CASE WHEN t.sentiment = 'Negative'           THEN 1 else 0 end neg_count,\n",
    "            CASE WHEN t.sentiment = 'Extremely Positive' THEN 1 else 0 end extremely_pos_count,\n",
    "            CASE WHEN t.sentiment = 'Extremely Negative' THEN 1 else 0 end extremely_neg_count\n",
    "     from train_table t \n",
    "     --where t.UserName=14675\n",
    ")R\n",
    "GROUP BY R.UserName\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = execute_query(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>extremely_pos_count</th>\n",
       "      <th>extremely_neg_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13994</th>\n",
       "      <td>14978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>14979</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>14980</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>14981</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>14983</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  pos_count  neg_count  extremely_pos_count   \n",
       "0             0          1          0                    0  \\\n",
       "1             1          2          0                    2   \n",
       "2             2          0          1                    0   \n",
       "3             3          1          0                    0   \n",
       "4             4          1          0                    1   \n",
       "...         ...        ...        ...                  ...   \n",
       "13994     14978          0          0                    0   \n",
       "13995     14979          1          0                    2   \n",
       "13996     14980          2          0                    0   \n",
       "13997     14981          2          0                    0   \n",
       "13998     14983          0          0                    0   \n",
       "\n",
       "       extremely_neg_count  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        1  \n",
       "4                        0  \n",
       "...                    ...  \n",
       "13994                    1  \n",
       "13995                    0  \n",
       "13996                    0  \n",
       "13997                    0  \n",
       "13998                    1  \n",
       "\n",
       "[13999 rows x 5 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Medium SQL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The sales department asks you to plot the positive sentiment rate in time\n",
    "\n",
    "The positive sentiment rate is defined as: \n",
    "\n",
    "    (count of positives + count of extremely positives) / [(count of negatives + count of extremely negatives) + (count of positives + count of extremely positives)]\n",
    "    \n",
    "requirement:\n",
    "\n",
    "1. positive rate column most be named pos_rate\n",
    "\n",
    "\n",
    "\n",
    "EXAMPLE SOLUTION:\n",
    "\n",
    "\n",
    "\ttweetAt\t    pos_rate\n",
    "0\t18-03-2020\t1.000000\n",
    "1\t21-03-2020\t1.000000\n",
    "...\n",
    "4\t18-03-2020\t0.777778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write query inside docstring\n",
    "query_3 = \"\"\"\n",
    "\n",
    "-- NOTA 1: este punto me causa algo de confusion, ya que se solicita trasar el sentimiento en el tiempo, \n",
    "--         eso me hace pensar que se debe agrupar por fecha. pero lo que me detiene es que en el ejemplo de reultado\n",
    "--         aparecen fechas repetidas. es por ello que al final no agrupé.\n",
    "\n",
    "-- NOTA 2: Descargue la info e hice varias pruebas:\n",
    "--         1. Si no agrupaba los resultados de la fórmula son: 0, 1 y erro al dividir entre cero. no me dan ningun valor de los de ejemplo\n",
    "--         2. La única forma que vi de que la formula me devuelva valores entre el 0.4 y el 0.7 fue agrupando por fecha y sacando el promedio.\n",
    "\n",
    "-- NOTA 3: cuando me encuentro casos así, para no tener dudas consulto con la persona que necesita la información para comprender el requerimeinto\n",
    "\n",
    "-- Nota 4: Para culminar este punto, opté por basarme en el resultado de ejemplo, por lo que no agrupe y solo aplique la fórmula.\n",
    "\n",
    "\n",
    "SELECT \n",
    "R.tweetAt,\n",
    "(R.pos_count + R.extremely_pos_count)/((neg_count + extremely_neg_count)+(R.pos_count + R.extremely_pos_count)) pos_rate\n",
    "\n",
    "FROM (\n",
    "     select \n",
    "                                                                           t.tweetAt,\n",
    "            CASE WHEN t.sentiment = 'Positive'           THEN 1 else 0 end pos_count,\n",
    "            CASE WHEN t.sentiment = 'Negative'           THEN 1 else 0 end neg_count,\n",
    "            CASE WHEN t.sentiment = 'Neutral'            THEN 1 else 0 end neutral_count,\n",
    "            CASE WHEN t.sentiment = 'Extremely Positive' THEN 1 else 0 end extremely_pos_count,\n",
    "            CASE WHEN t.sentiment = 'Extremely Negative' THEN 1 else 0 end extremely_neg_count\n",
    "     from train_table t \n",
    ")R\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = execute_query(query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetAt</th>\n",
       "      <th>pos_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tweetAt  pos_rate\n",
       "0      16-03-2020       NaN\n",
       "1      16-03-2020       1.0\n",
       "2      16-03-2020       1.0\n",
       "3      16-03-2020       1.0\n",
       "4      16-03-2020       0.0\n",
       "...           ...       ...\n",
       "41152  14-04-2020       NaN\n",
       "41153  14-04-2020       0.0\n",
       "41154  14-04-2020       1.0\n",
       "41155  14-04-2020       NaN\n",
       "41156  14-04-2020       0.0\n",
       "\n",
       "[41157 rows x 2 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: SQL + pandas + plotting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You are tasked to deliver a simple plot a time series of the answer of Question 3\n",
    "\n",
    "Requirements:\n",
    "    1. the proceedure most take the result_3 variable as input and return nothing\n",
    "    2. the plot most be resampled by day aggregated by the mean\n",
    "    3. no other library like matplotlib can be used only pandas plot interface pd.Series.plot()\n",
    "    4. the date column from result_3 most be parsed into datetime format using pandas built in method.\n",
    "    5. the proceedure must not transform or modify the original result_3 variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Expected result:</p>\n",
    "<img src=\"data/plot_result.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(df : pd.DataFrame):\n",
    "    #TODO: YOUR CODE GOES HERE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>set_type</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>extremely_pos_count</th>\n",
       "      <th>extremely_neg_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7484</td>\n",
       "      <td>9996</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3593</td>\n",
       "      <td>2338</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12246</td>\n",
       "      <td>5091</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5237</td>\n",
       "      <td>10266</td>\n",
       "      <td>None</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14067</td>\n",
       "      <td>3014</td>\n",
       "      <td>None</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>41152</td>\n",
       "      <td>10280</td>\n",
       "      <td>14781</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>41153</td>\n",
       "      <td>10605</td>\n",
       "      <td>1158</td>\n",
       "      <td>None</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>41154</td>\n",
       "      <td>4884</td>\n",
       "      <td>1029</td>\n",
       "      <td>None</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>You know itÂs getting tough when @KameronWild...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>41155</td>\n",
       "      <td>9052</td>\n",
       "      <td>7736</td>\n",
       "      <td>None</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>41156</td>\n",
       "      <td>13519</td>\n",
       "      <td>6644</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  UserName  ScreenName                      Location     TweetAt   \n",
       "0          0      7484        9996                        London  16-03-2020  \\\n",
       "1          1      3593        2338                            UK  16-03-2020   \n",
       "2          2     12246        5091                     Vagabonds  16-03-2020   \n",
       "3          3      5237       10266                          None  16-03-2020   \n",
       "4          4     14067        3014                          None  16-03-2020   \n",
       "...      ...       ...         ...                           ...         ...   \n",
       "41152  41152     10280       14781  Wellington City, New Zealand  14-04-2020   \n",
       "41153  41153     10605        1158                          None  14-04-2020   \n",
       "41154  41154      4884        1029                          None  14-04-2020   \n",
       "41155  41155      9052        7736                          None  14-04-2020   \n",
       "41156  41156     13519        6644  i love you so much || he/him  14-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment   \n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \\\n",
       "1      advice Talk to your neighbours family to excha...            Positive   \n",
       "2      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3      My food stock is not the only one which is emp...            Positive   \n",
       "4      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "...                                                  ...                 ...   \n",
       "41152  Airline pilots offering to stock supermarket s...             Neutral   \n",
       "41153  Response to complaint not provided citing COVI...  Extremely Negative   \n",
       "41154  You know itÂs getting tough when @KameronWild...            Positive   \n",
       "41155  Is it wrong that the smell of hand sanitizer i...             Neutral   \n",
       "41156  @TartiiCat Well new/used Rift S are going for ...            Negative   \n",
       "\n",
       "      set_type  pos_count  neg_count  extremely_pos_count  extremely_neg_count  \n",
       "0        train          0          0                    0                    0  \n",
       "1        train          1          0                    0                    0  \n",
       "2        train          1          0                    0                    0  \n",
       "3        train          1          0                    0                    0  \n",
       "4        train          0          0                    0                    1  \n",
       "...        ...        ...        ...                  ...                  ...  \n",
       "41152    train          0          0                    0                    0  \n",
       "41153    train          0          0                    0                    1  \n",
       "41154    train          1          0                    0                    0  \n",
       "41155    train          0          0                    0                    0  \n",
       "41156    train          0          1                    0                    0  \n",
       "\n",
       "[41157 rows x 12 columns]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_sql_query() missing 1 required positional argument: 'con'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[345], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_3\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[343], line 3\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(df : pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#TODO: YOUR CODE GOES HERE\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_3\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_sql_query() missing 1 required positional argument: 'con'"
     ]
    }
   ],
   "source": [
    "plot(result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Object Oriented Programming + Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You are tasked to build a csv data reader from scratch.\n",
    "As a major constraint you cannot use any third party library like pandas or another csv reader. \n",
    "***Only built in standard python libraries are allowed.***\n",
    "\n",
    "\n",
    "The problem is that the csv data provided has many mistakes and the user requires to keep the amount of data possible.\n",
    "The user does not wants to drop rows or columns with mistakes but may replace mistakes with \"NaN\" values\n",
    "\n",
    "REQUIREMENTS:\n",
    "\n",
    "I. the csv must be contained inside a class named Data and it must have:\n",
    "    1. must persist the loaded csv data in a local parameter called data\n",
    "    2. infer_dtypes(data_object: object) : to infer the data types of each column distinguishing between (date, integer, or float)\n",
    "    3. describe() method that returns basic statistics of each numerical column including mean, standard deviation, maximum value, minimum value.\n",
    "II. You most build a class called DataReader with the following required methods:\n",
    "    1. read_csv(file_path : Str) : to load the raw csv file and return a Data class\n",
    "\n",
    "\n",
    "You can assume:\n",
    "    1. all columns have different errors at different rows\n",
    "    2. you can delete rows ONLY if necesary and you need to comment why\n",
    "    3. the more rows you drop the less points you will obtain\n",
    "\n",
    "\n",
    "If it is not specified in this instructions you are free to assume but explain your assumptions.\n",
    "Use best practices and comment your code as best as possible.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# example template for class\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        # CODE GOES HERE\n",
    "\n",
    "\n",
    "\n",
    "    def infer_dtypes(self) -> dict:\n",
    "        # CODE GOES HERE\n",
    "\n",
    "        return \n",
    "\n",
    "    def describe(self) -> dict:\n",
    "        # CODE GOES HERE \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola mundo\n",
      "['10/9/2016 0:00', 54, 21.93, 21.0, 22.8]\n",
      "['10/10/2016 0:00', 52, 21.77, 20.4, 23.6]\n",
      "['10/11/2016 0:00', 51, 21.36, 19.9, 23.0]\n",
      "['10/12/2016 0:00', 51, 21.44, 20.0, 23.6]\n",
      "['10/13/2016 0:00', 52, 21.22, 20.1, 22.3]\n",
      "['10/14/2016 0:00', 52, 21.02, 19.6, 22.6]\n",
      "['10/15/2016 0:00', 53, 21.4, 20.3, 22.5]\n",
      "['10/16/2016 0:00', 53, 21.43, 20.0, 23.0]\n",
      "['10/17/2016 0:00', 53, 21.67, 20.5, 22.7]\n",
      "['10/18/2016 0:00', 54, 21.75, 20.6, 23.1]\n",
      "['10/19/2016 0:00', 53, 21.22, 20.2, 22.7]\n",
      "['10/20/2016 0:00', 52, 21.25, 20.2, 22.8]\n",
      "['10/21/2016 0:00', 50, 21.09, 19.4, 23.1]\n",
      "['10/22/2016 0:00', 49, 20.86, 19.5, 22.5]\n",
      "['10/23/2016 0:00', 49, 20.51, 19.0, 22.2]\n",
      "['10/24/2016 0:00', 49, 20.38, 19.1, 22.3]\n",
      "['10/25/2016 0:00', 50, 20.8, 19.7, 22.3]\n",
      "['10/26/2016 0:00', 50, 21.14, 20.0, 'NaN']\n",
      "['10/27/2016 0:00', 50, 21.15, 20.1, 22.5]\n",
      "['10/28/2016 0:00', 51, 21.57, 20.6, 'NaN']\n",
      "['10/29/2016 0:00', 52, 'NaN', 21.0, 22.9]\n",
      "['10/30/2016 0:00', 52, 21.88, 21.0, 22.9]\n",
      "['10/31/2016 0:00', 53, 21.72, 20.7, 22.8]\n",
      "['11/1/2016 0:00', 52, 21.53, 20.2, 'NaN']\n",
      "['11/2/2016 0:00', 50, 21.11, 19.8, 22.6]\n",
      "['11/3/2016 0:00', 49, 20.31, 18.7, 22.4]\n",
      "['NaN', 50, 20.76, 19.4, 23.1]\n",
      "['11/5/2016 0:00', 50, 20.14, 18.5, 22.4]\n",
      "['11/6/2016 0:00', 50, 19.4, 17.5, 22.3]\n",
      "['11/7/2016 0:00', 48, 19.58, 17.8, 22.3]\n",
      "['11/8/2016 0:00', 47, 19.31, 17.6, 22.0]\n",
      "['11/9/2016 0:00', 48, 19.78, 18.3, 22.0]\n",
      "['11/10/2016 0:00', 47, 20.3, 19.0, 22.3]\n",
      "['11/11/2016 0:00', 47, 20.28, 18.9, 22.6]\n",
      "['NaN', 47, 20.83, 19.6, 22.4]\n",
      "['11/13/2016 0:00', 48, 21.0, 20.0, 22.6]\n",
      "['11/14/2016 0:00', 48, 20.95, 19.9, 22.5]\n",
      "['11/15/2016 0:00', 49, 21.64, 20.8, 22.6]\n",
      "['11/16/2016 0:00', 50, 21.7, 20.6, 22.6]\n",
      "['11/17/2016 0:00', 49, 20.98, 19.7, 22.0]\n",
      "['11/18/2016 0:00', 48, 20.07, 18.5, 22.0]\n",
      "['11/19/2016 0:00', 47, 19.35, 17.5, 21.8]\n",
      "['11/20/2016 0:00', 47, 19.12, 17.7, 21.4]\n",
      "['11/21/2016 0:00', 47, 19.43, 17.8, 'NaN']\n",
      "['NaN', 47, 19.94, 18.3, 21.8]\n",
      "['11/23/2016 0:00', 47, 19.96, 18.2, 22.0]\n",
      "['11/24/2016 0:00', 48, 20.16, 18.5, 22.0]\n",
      "['NaN', 49, 19.83, 18.4, 21.6]\n",
      "['11/26/2016 0:00', 48, 19.69, 18.1, 22.3]\n",
      "['11/27/2016 0:00', 48, 19.39, 17.6, 21.5]\n",
      "['11/28/2016 0:00', 48, 20.27, 17.9, 23.1]\n",
      "['11/29/2016 0:00', 47, 18.72, 16.3, 22.4]\n",
      "['11/30/2016 0:00', 44, 18.11, 14.9, 21.4]\n",
      "['12/1/2016 0:00', 41, 18.04, 15.6, 19.7]\n",
      "['12/2/2016 0:00', 41, 18.58, 16.1, 21.6]\n",
      "['12/3/2016 0:00', 42, 18.86, 17.5, 21.0]\n",
      "['12/4/2016 0:00', 42, 18.68, 16.9, 20.7]\n",
      "['12/5/2016 0:00', 42, 18.83, 16.8, 21.3]\n",
      "['12/6/2016 0:00', 42, 19.24, 18.0, 21.6]\n",
      "['12/7/2016 0:00', 45, 20.16, 19.0, 'NaN']\n",
      "['NaN', 47, 20.87, 20.0, 22.0]\n",
      "['NaN', 48, 21.18, 20.4, 22.2]\n",
      "['NaN', 49, 20.92, 20.2, 22.0]\n",
      "['NaN', 49, 19.99, 19.0, 21.6]\n",
      "['NaN', 49, 19.1, 17.8, 21.2]\n",
      "['NaN', 49, 'NaN', 18.7, 21.8]\n",
      "['NaN', 49, 20.44, 19.3, 21.6]\n",
      "['NaN', 50, 20.48, 19.2, 22.3]\n",
      "['NaN', 50, 21.24, 19.5, 24.8]\n",
      "['NaN', 48, 23.57, 19.8, 25.2]\n",
      "['NaN', 48, 21.12, 19.1, 23.0]\n",
      "['NaN', 47, 20.16, 18.2, 22.1]\n",
      "['12/21/2016 0:00', 47, 20.07, 18.9, 22.2]\n",
      "['12/22/2016 0:00', 47, 19.56, 17.6, 21.7]\n",
      "['12/23/2016 0:00', 47, 18.88, 17.1, 'NaN']\n",
      "['12/24/2016 0:00', 47, 19.15, 17.3, 21.7]\n",
      "['12/25/2016 0:00', 47, 19.77, 18.4, 22.0]\n",
      "['12/26/2016 0:00', 48, 19.61, 'NaN', 21.7]\n",
      "['12/27/2016 0:00', 45, 'NaN', 17.2, 21.3]\n",
      "['12/28/2016 0:00', 43, 18.48, 16.1, 21.2]\n",
      "['12/29/2016 0:00', 41, 18.74, 16.1, 21.4]\n",
      "['12/30/2016 0:00', 42, 18.48, 16.4, 21.1]\n",
      "['12/31/2016 0:00', 44, 19.14, 17.2, 22.1]\n",
      "['1/1/2017 0:00', 45, 19.32, 17.7, 21.4]\n",
      "['1/2/2017 0:00', 46, 18.76, 16.9, 21.4]\n",
      "['1/3/2017 0:00', 45, 18.64, 16.5, 21.5]\n",
      "['1/4/2017 0:00', 43, 18.52, 16.3, 21.5]\n",
      "['1/5/2017 0:00', 43, 18.48, 16.5, 21.2]\n",
      "['1/6/2017 0:00', 41, 18.6, 16.2, 21.7]\n",
      "['1/7/2017 0:00', 44, 19.38, 18.0, 22.0]\n",
      "['1/8/2017 0:00', 45, 20.13, 18.9, 22.1]\n",
      "['1/9/2017 0:00', 47, 20.07, 18.3, 23.2]\n",
      "['1/10/2017 0:00', 45, 'NaN', 17.6, 22.8]\n",
      "['1/11/2017 0:00', 46, 'NaN', 18.0, 22.9]\n",
      "['1/12/2017 0:00', 45, 'NaN', 17.6, 22.8]\n",
      "['1/13/2017 0:00', 43, 'NaN', 17.1, 22.1]\n",
      "['1/14/2017 0:00', 42, 18.89, 16.3, 23.0]\n",
      "['1/15/2017 0:00', 43, 19.41, 17.4, 23.2]\n",
      "['1/16/2017 0:00', 45, 20.09, 18.1, 23.4]\n",
      "['1/17/2017 0:00', 45, 19.84, 17.5, 23.0]\n",
      "['1/18/2017 0:00', 45, 19.61, 17.8, 23.2]\n",
      "['1/19/2017 0:00', 43, 19.61, 17.4, 23.4]\n",
      "['1/20/2017 0:00', 42, 19.43, 17.3, 23.0]\n",
      "['1/21/2017 0:00', 40, 19.3, 16.6, 22.9]\n",
      "['1/22/2017 0:00', 39, 20.45, 18.1, 23.3]\n",
      "['1/23/2017 0:00', 39, 20.55, 18.2, 23.3]\n",
      "['1/24/2017 0:00', 39, 19.92, 17.7, 23.8]\n",
      "['1/25/2017 0:00', 39, 21.08, 17.8, 24.7]\n",
      "['1/26/2017 0:00', 38, 19.9, 18.1, 22.8]\n",
      "['1/27/2017 0:00', 37, 19.15, 16.8, 23.2]\n",
      "['1/28/2017 0:00', 39, 19.87, 18.1, 22.7]\n",
      "['1/29/2017 0:00', 39, 19.53, 17.9, 22.2]\n",
      "['1/30/2017 0:00', 42, 19.89, 18.1, 23.0]\n",
      "['1/31/2017 0:00', 43, 20.3, 18.7, 23.3]\n",
      "['2/1/2017 0:00', 45, 20.62, 19.1, 23.3]\n",
      "['2/2/2017 0:00', 46, 20.73, 19.4, 23.1]\n",
      "['2/3/2017 0:00', 46, 20.23, 18.8, 22.6]\n",
      "['2/4/2017 0:00', 45, 19.83, 18.0, 22.5]\n",
      "['2/5/2017 0:00', 43, 19.39, 17.8, 22.7]\n",
      "['2/6/2017 0:00', 42, 19.13, 17.3, 22.3]\n",
      "['2/7/2017 0:00', 43, 19.73, 17.9, 22.2]\n",
      "['2/8/2017 0:00', 43, 19.76, 18.2, 22.8]\n",
      "['2/9/2017 0:00', 42, 19.14, 17.8, 22.4]\n",
      "['2/10/2017 0:00', 40, 18.9, 16.8, 22.5]\n",
      "['2/11/2017 0:00', 39, 18.96, 16.5, 22.7]\n",
      "['2/12/2017 0:00', 38, 19.73, 16.8, 23.2]\n",
      "['2/13/2017 0:00', 38, 19.99, 18.2, 23.4]\n",
      "['2/14/2017 0:00', 39, 19.81, 18.1, 23.2]\n",
      "['2/15/2017 0:00', 41, 22.07, 20.0, 24.3]\n",
      "['2/16/2017 0:00', 42, 20.95, 19.4, 23.4]\n",
      "['2/17/2017 0:00', 43, 21.1, 19.8, 23.4]\n",
      "['2/18/2017 0:00', 43, 20.73, 19.5, 22.5]\n",
      "['2/19/2017 0:00', 43, 20.9, 19.5, 23.5]\n",
      "['2/20/2017 0:00', 45, 21.21, 19.9, 23.1]\n",
      "['2/21/2017 0:00', 46, 21.18, 19.6, 23.0]\n",
      "['2/22/2017 0:00', 47, 20.77, 19.4, 22.9]\n",
      "['2/23/2017 0:00', 47, 20.83, 19.4, 22.6]\n",
      "['2/24/2017 0:00', 45, 20.55, 18.8, 23.5]\n",
      "['2/25/2017 0:00', 44, 20.34, 18.7, 22.5]\n",
      "['2/26/2017 0:00', 45, 20.49, 19.1, 22.5]\n",
      "['2/27/2017 0:00', 45, 20.37, 18.5, 22.5]\n",
      "['2/28/2017 0:00', 44, 19.64, 17.7, 22.5]\n",
      "['3/1/2017 0:00', 44, 19.67, 17.8, 22.8]\n",
      "['3/2/2017 0:00', 43, 20.47, 18.2, 23.0]\n",
      "['3/3/2017 0:00', 44, 20.0, 18.1, 22.5]\n",
      "['3/4/2017 0:00', 45, 20.66, 19.1, 23.0]\n",
      "['3/5/2017 0:00', 44, 20.52, 18.8, 22.8]\n",
      "['3/6/2017 0:00', 43, 20.4, 18.7, 22.9]\n",
      "['3/7/2017 0:00', 43, 20.36, 18.9, 22.8]\n",
      "['3/8/2017 0:00', 44, 20.74, 19.5, 23.1]\n",
      "['3/9/2017 0:00', 46, 21.82, 20.2, 23.6]\n",
      "['3/10/2017 0:00', 46, 21.59, 20.6, 23.3]\n",
      "['3/11/2017 0:00', 47, 21.59, 20.5, 22.8]\n",
      "['3/12/2017 0:00', 47, 21.66, 20.6, 23.2]\n",
      "['3/13/2017 0:00', 46, 22.1, 20.2, 24.2]\n",
      "['3/14/2017 0:00', 47, 21.53, 20.4, 23.2]\n",
      "['3/15/2017 0:00', 47, 22.09, 20.4, 24.1]\n",
      "['3/16/2017 0:00', 48, 21.29, 20.0, 23.0]\n",
      "['3/17/2017 0:00', 47, 20.83, 19.2, 23.5]\n",
      "['3/18/2017 0:00', 48, 21.36, 19.9, 23.5]\n",
      "['3/19/2017 0:00', 51, 21.85, 20.3, 25.3]\n",
      "['3/20/2017 0:00', 52, 22.41, 21.4, 23.7]\n",
      "['3/21/2017 0:00', 50, 21.52, 19.9, 23.4]\n",
      "['3/22/2017 0:00', 48, 20.39, 18.5, 22.7]\n",
      "['3/23/2017 0:00', 48, 19.72, 18.0, 23.0]\n",
      "['3/24/2017 0:00', 46, 19.97, 18.1, 23.1]\n",
      "['3/25/2017 0:00', 45, 20.93, 18.9, 23.5]\n",
      "['3/27/2017 0:00', 44, 21.53, 20.2, 23.7]\n",
      "['3/28/2017 0:00', 44, 21.25, 20.1, 23.0]\n",
      "['3/29/2017 0:00', 46, 21.38, 20.2, 23.2]\n",
      "['3/30/2017 0:00', 47, 21.93, 20.6, 23.2]\n",
      "['3/31/2017 0:00', 48, 22.41, 21.3, 23.5]\n",
      "['4/1/2017 0:00', 47, 22.33, 20.8, 23.8]\n",
      "['4/2/2017 0:00', 47, 22.16, 20.6, 23.7]\n",
      "['4/3/2017 0:00', 47, 21.93, 20.8, 23.1]\n",
      "['4/4/2017 0:00', 47, 21.87, 20.7, 23.1]\n",
      "['4/5/2017 0:00', 46, 21.8, 20.3, 23.5]\n",
      "['4/6/2017 0:00', 44, 21.87, 20.0, 23.5]\n",
      "['4/7/2017 0:00', 44, 22.33, 20.8, 24.2]\n",
      "['4/8/2017 0:00', 44, 22.84, 20.8, 24.7]\n",
      "['4/9/2017 0:00', 45, 23.06, 21.7, 24.8]\n",
      "['4/10/2017 0:00', 45, 22.84, 21.6, 24.0]\n",
      "['4/11/2017 0:00', 45, 22.46, 20.9, 23.5]\n",
      "['4/12/2017 0:00', 46, 22.12, 20.9, 23.7]\n",
      "['4/13/2017 0:00', 45, 21.9, 20.7, 23.3]\n",
      "['4/14/2017 0:00', 44, 21.83, 20.6, 23.3]\n",
      "['4/15/2017 0:00', 44, 22.23, 21.0, 23.5]\n",
      "['4/16/2017 0:00', 44, 22.27, 21.0, 23.3]\n",
      "['4/17/2017 0:00', 44, 22.41, 21.2, 23.2]\n",
      "['4/18/2017 0:00', 43, 22.25, 20.8, 24.1]\n",
      "['4/19/2017 0:00', 42, 22.46, 21.2, 23.8]\n",
      "['4/20/2017 0:00', 43, 22.46, 21.7, 23.6]\n",
      "['4/21/2017 0:00', 43, 22.16, 21.3, 23.1]\n",
      "['4/22/2017 0:00', 43, 23.12, 21.6, 25.0]\n",
      "['4/23/2017 0:00', 43, 23.16, 21.5, 24.7]\n",
      "['4/24/2017 0:00', 43, 22.08, 21.2, 23.1]\n",
      "['4/25/2017 0:00', 42, 21.03, 19.7, 23.1]\n",
      "['4/26/2017 0:00', 40, 20.71, 18.8, 23.3]\n",
      "['4/27/2017 0:00', 39, 21.02, 19.5, 23.3]\n",
      "['4/28/2017 0:00', 40, 21.17, 19.6, 23.6]\n",
      "['4/29/2017 0:00', 41, 21.44, 20.2, 23.2]\n",
      "['4/30/2017 0:00', 41, 21.62, 20.5, 22.9]\n",
      "['5/1/2017 0:00', 42, 21.8, 20.6, 23.4]\n",
      "['5/2/2017 0:00', 43, 22.08, 20.6, 23.7]\n",
      "['5/3/2017 0:00', 42, 21.97, 21.2, 23.2]\n",
      "['5/4/2017 0:00', 42, 21.76, 20.6, 23.5]\n",
      "['5/5/2017 0:00', 41, 21.92, 20.8, 23.5]\n",
      "['5/6/2017 0:00', 41, 22.16, 21.2, 23.4]\n",
      "['5/7/2017 0:00', 42, 22.64, 21.0, 24.2]\n",
      "['5/8/2017 0:00', 42, 22.94, 21.8, 24.3]\n",
      "['5/9/2017 0:00', 41, 22.71, 21.6, 24.3]\n",
      "['5/10/2017 0:00', 40, 22.56, 20.8, 24.4]\n",
      "['5/11/2017 0:00', 41, 22.58, 21.7, 23.6]\n",
      "['5/12/2017 0:00', 43, 22.84, 22.4, 23.5]\n",
      "['5/13/2017 0:00', 43, 22.63, 21.8, 23.7]\n",
      "['5/14/2017 0:00', 44, 23.09, 22.0, 24.2]\n",
      "['5/15/2017 0:00', 45, 22.47, 22.0, 23.3]\n",
      "['5/16/2017 0:00', 47, 22.41, 22.0, 23.3]\n",
      "['5/17/2017 0:00', 48, 22.2, 21.6, 23.0]\n",
      "['5/18/2017 0:00', 47, 22.15, 20.8, 23.3]\n",
      "['5/19/2017 0:00', 46, 22.98, 21.5, 24.6]\n",
      "['5/20/2017 0:00', 46, 22.61, 21.8, 23.6]\n",
      "['5/21/2017 0:00', 46, 22.71, 21.0, 24.2]\n",
      "['5/22/2017 0:00', 46, 23.77, 22.5, 25.4]\n",
      "['5/23/2017 0:00', 47, 23.84, 23.1, 24.8]\n",
      "['5/24/2017 0:00', 48, 24.82, 23.3, 26.5]\n",
      "['5/25/2017 0:00', 48, 26.34, 25.1, 28.1]\n",
      "['5/26/2017 0:00', 49, 26.78, 25.3, 28.5]\n",
      "['5/27/2017 0:00', 48, 26.56, 25.6, 27.5]\n",
      "['5/28/2017 0:00', 49, 25.02, 24.3, 25.7]\n",
      "['5/29/2017 0:00', 50, 24.36, 24.0, 25.0]\n",
      "['5/30/2017 0:00', 50, 23.41, 23.0, 24.0]\n",
      "['5/31/2017 0:00', 51, 23.17, 22.7, 23.8]\n",
      "['6/1/2017 0:00', 51, 23.82, 22.8, 25.3]\n",
      "['6/2/2017 0:00', 51, 24.48, 24.1, 24.9]\n",
      "['6/3/2017 0:00', 51, 24.38, 23.5, 25.6]\n",
      "['6/4/2017 0:00', 50, 23.85, 23.3, 24.6]\n",
      "['6/5/2017 0:00', 50, 22.33, 21.5, 23.6]\n",
      "['6/6/2017 0:00', 51, 21.77, 21.1, 22.5]\n",
      "['6/7/2017 0:00', 50, 21.97, 20.7, 22.7]\n",
      "['6/8/2017 0:00', 51, 22.14, 21.8, 22.6]\n",
      "['6/9/2017 0:00', 51, 22.66, 21.6, 23.9]\n",
      "['6/10/2017 0:00', 52, 23.14, 22.5, 23.8]\n",
      "['6/11/2017 0:00', 53, 23.52, 22.8, 24.3]\n",
      "['6/12/2017 0:00', 53, 23.31, 22.7, 24.2]\n",
      "['6/13/2017 0:00', 52, 23.35, 22.2, 24.9]\n",
      "['6/14/2017 0:00', 52, 24.84, 23.5, 26.7]\n",
      "['6/15/2017 0:00', 51, 25.46, 24.8, 26.4]\n",
      "['6/16/2017 0:00', 51, 25.06, 24.1, 26.5]\n",
      "['6/17/2017 0:00', 50, 25.88, 24.5, 27.9]\n",
      "['6/18/2017 0:00', 50, 27.35, 26.1, 28.7]\n",
      "['6/19/2017 0:00', 51, 28.97, 27.6, 30.7]\n",
      "['6/20/2017 0:00', 51, 29.21, 27.4, 30.8]\n",
      "['6/21/2017 0:00', 51, 28.97, 26.8, 31.1]\n",
      "['6/22/2017 0:00', 52, 28.81, 28.2, 30.2]\n",
      "['6/23/2017 0:00', 50, 26.78, 26.2, 28.2]\n",
      "['6/24/2017 0:00', 51, 25.74, 25.1, 26.5]\n",
      "['6/25/2017 0:00', 52, 25.11, 24.7, 25.7]\n",
      "['6/26/2017 0:00', 51, 24.81, 23.8, 25.6]\n",
      "['6/27/2017 0:00', 54, 24.47, 24.1, 24.8]\n",
      "['6/28/2017 0:00', 54, 24.07, 23.2, 24.8]\n",
      "['6/29/2017 0:00', 55, 22.15, 21.6, 23.2]\n",
      "['6/30/2017 0:00', 54, 22.52, 21.6, 23.3]\n",
      "['7/1/2017 0:00', 53, 23.63, 22.8, 25.3]\n",
      "['7/2/2017 0:00', 52, 25.02, 23.9, 26.8]\n",
      "['7/3/2017 0:00', 52, 24.9, 23.9, 26.0]\n",
      "['7/4/2017 0:00', 52, 24.7, 24.1, 25.6]\n",
      "['7/5/2017 0:00', 52, 25.5, 24.0, 27.5]\n",
      "['7/6/2017 0:00', 52, 27.06, 25.8, 28.4]\n",
      "['7/7/2017 0:00', 52, 27.19, 26.2, 28.2]\n",
      "['7/8/2017 0:00', 51, 27.06, 26.4, 27.7]\n",
      "['7/9/2017 0:00', 51, 27.61, 26.4, 29.3]\n",
      "['7/10/2017 0:00', 51, 27.07, 26.4, 28.3]\n",
      "['7/11/2017 0:00', 52, 25.17, 24.1, 26.6]\n",
      "['7/12/2017 0:00', 52, 23.76, 23.0, 24.4]\n",
      "['7/13/2017 0:00', 52, 24.06, 23.5, 24.8]\n",
      "['7/14/2017 0:00', 52, 24.58, 24.0, 25.6]\n",
      "['7/15/2017 0:00', 52, 24.14, 23.8, 24.9]\n",
      "['7/16/2017 0:00', 54, 24.45, 23.7, 25.7]\n",
      "['7/17/2017 0:00', 52, 24.89, 23.8, 25.9]\n",
      "['7/18/2017 0:00', 51, 25.65, 24.8, 26.5]\n",
      "['7/19/2017 0:00', 52, 25.75, 25.3, 26.4]\n",
      "['7/20/2017 0:00', 53, 25.12, 24.5, 26.0]\n",
      "['7/21/2017 0:00', 53, 23.89, 22.8, 24.7]\n",
      "['7/23/2017 0:00', 56, 21.9, 21.9, 21.9]\n",
      "['7/24/2017 0:00', 55, 22.51, 21.4, 24.1]\n",
      "['7/25/2017 0:00', 54, 24.31, 23.1, 26.1]\n",
      "['7/26/2017 0:00', 54, 24.56, 23.8, 25.5]\n",
      "['7/27/2017 0:00', 55, 23.61, 22.9, 24.4]\n",
      "['7/28/2017 0:00', 55, 22.75, 22.3, 23.5]\n",
      "['7/29/2017 0:00', 57, 22.42, 22.0, 22.7]\n",
      "['7/30/2017 0:00', 56, 22.57, 22.0, 23.5]\n",
      "['7/31/2017 0:00', 56, 22.62, 22.0, 23.0]\n",
      "['8/1/2017 0:00', 56, 22.44, 21.9, 22.9]\n",
      "['8/2/2017 0:00', 57, 22.02, 21.7, 22.7]\n",
      "['8/12/2017 0:00', 57, 23.05, 22.8, 23.3]\n",
      "['8/13/2017 0:00', 56, 22.8, 22.0, 23.6]\n",
      "['8/14/2017 0:00', 56, 22.35, 21.9, 23.3]\n",
      "['8/15/2017 0:00', 57, 22.77, 21.7, 24.6]\n",
      "['8/16/2017 0:00', 56, 23.11, 22.2, 24.0]\n",
      "['8/17/2017 0:00', 57, 23.61, 22.8, 24.7]\n",
      "['8/18/2017 0:00', 58, 23.55, 23.0, 24.1]\n",
      "['8/19/2017 0:00', 57, 22.74, 21.9, 24.2]\n",
      "['8/20/2017 0:00', 56, 22.26, 21.9, 23.0]\n",
      "['8/21/2017 0:00', 57, 22.85, 21.8, 24.4]\n",
      "['8/22/2017 0:00', 58, 23.98, 23.6, 24.3]\n",
      "['8/23/2017 0:00', 59, 24.13, 23.6, 25.0]\n",
      "['8/24/2017 0:00', 58, 23.76, 23.0, 24.7]\n",
      "['8/25/2017 0:00', 57, 23.47, 22.6, 24.4]\n",
      "['8/26/2017 0:00', 57, 23.99, 22.8, 25.3]\n",
      "['8/27/2017 0:00', 56, 24.96, 23.7, 27.0]\n",
      "['8/28/2017 0:00', 57, 26.21, 25.0, 28.2]\n",
      "['8/29/2017 0:00', 57, 25.62, 25.0, 26.8]\n",
      "['8/30/2017 0:00', 58, 23.91, 23.3, 25.0]\n",
      "['8/31/2017 0:00', 57, 23.17, 21.8, 25.0]\n",
      "['9/1/2017 0:00', 55, 23.21, 22.2, 24.7]\n",
      "['9/2/2017 0:00', 54, 23.26, 22.2, 24.7]\n",
      "['9/3/2017 0:00', 55, 22.3, 21.7, 23.3]\n",
      "['9/4/2017 0:00', 57, 22.34, 21.7, 23.2]\n",
      "['9/5/2017 0:00', 58, 22.68, 22.3, 23.2]\n",
      "['9/6/2017 0:00', 57, 22.65, 21.9, 23.8]\n",
      "['9/7/2017 0:00', 56, 22.35, 22.1, 22.7]\n",
      "['9/8/2017 0:00', 57, 22.34, 21.7, 23.5]\n",
      "['9/9/2017 0:00', 56, 22.24, 21.4, 23.5]\n",
      "['9/10/2017 0:00', 57, 21.52, 21.1, 22.4]\n",
      "['9/11/2017 0:00', 56, 21.62, 20.6, 22.7]\n",
      "['9/12/2017 0:00', 56, 21.73, 20.7, 22.6]\n",
      "['9/13/2017 0:00', 55, 22.12, 21.1, 23.6]\n",
      "['9/14/2017 0:00', 53, 22.13, 21.1, 23.1]\n",
      "['9/15/2017 0:00', 53, 21.82, 20.8, 22.8]\n",
      "['9/16/2017 0:00', 53, 21.2, 20.4, 21.8]\n",
      "['9/17/2017 0:00', 53, 21.27, 20.3, 22.3]\n",
      "['9/18/2017 0:00', 53, 21.38, 20.3, 22.2]\n",
      "['9/19/2017 0:00', 52, 22.13, 20.7, 23.8]\n",
      "['9/20/2017 0:00', 53, 22.15, 21.4, 22.7]\n",
      "['9/21/2017 0:00', 54, 21.87, 21.3, 22.5]\n",
      "['9/22/2017 0:00', 53, 21.56, 20.5, 22.5]\n",
      "['9/23/2017 0:00', 54, 22.44, 21.5, 24.1]\n",
      "['9/24/2017 0:00', 54, 22.24, 21.7, 22.5]\n",
      "['9/25/2017 0:00', 56, 21.93, 21.5, 22.4]\n",
      "['9/26/2017 0:00', 57, 21.89, 21.4, 22.5]\n",
      "['9/27/2017 0:00', 57, 21.9, 21.5, 22.2]\n",
      "['9/28/2017 0:00', 57, 22.22, 21.5, 23.6]\n",
      "['9/29/2017 0:00', 57, 22.05, 21.6, 22.6]\n",
      "['9/30/2017 0:00', 57, 21.45, 20.9, 22.0]\n",
      "['10/1/2017 0:00', 58, 21.4, 21.2, 21.8]\n",
      "['10/2/2017 0:00', 59, 21.54, 21.1, 22.0]\n",
      "['10/3/2017 0:00', 56, 21.4, 20.6, 22.4]\n",
      "['10/4/2017 0:00', 55, 20.87, 20.1, 21.9]\n",
      "['10/5/2017 0:00', 56, 21.34, 20.6, 22.8]\n",
      "['10/6/2017 0:00', 54, 21.34, 20.3, 22.3]\n",
      "['10/7/2017 0:00', 55, 21.34, 20.6, 21.9]\n",
      "['10/8/2017 0:00', 55, 21.81, 20.6, 23.5]\n",
      "['10/9/2017 0:00', 54, 22.23125, 22.2, 22.3]\n"
     ]
    }
   ],
   "source": [
    "# example template for class\n",
    "\n",
    "# COMENTARIOS:\n",
    "# investigando, observo que la libreria \"csv\" es una estándar de python y por eso la usaré. fuente: https://empresas.blogthinkbig.com/python-5-formas-de-cargar-datos-csv-proyectos-machine-learning/  en la parte que dice \"1. Usando el módulo csv de la librería estándar\".\n",
    "# La información descrita en el problema habla de \"los datos csv proporcionados tienen muchos errores\", buscando en el zip \"prueba_ing_datos-main.zip\" hay un archivo .csv así que imagino debo usar ese \"C:...\\prueba_ing_datos-main\\data\\malformed_dataset.csv\"\n",
    "# \n",
    "\n",
    "# Resume de lo logrado: no fue mucho lo logrado ya que mi nivel en python es bastante básico. 1. los campos vaciós de las fechas se reemplazó por \"NaN\". 2. se logró cargar los registros del csv. 3. se log´ro leer y condicionar lo leído. 4. lo último que pude hacer fue convertir la columna Humidity en numérico. La fecha no la termine pero se que es algo con %c mes numerico  (1...12),%e día numerico  (1...12), %Y año numeroco 4 dig\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "from datetime import datetime \n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        # CODE GOES HERE\n",
    "\n",
    "        #ver si ejecuta\n",
    "        print (\"Hola mundo\")\n",
    "\n",
    "        # se crea la variable datos que contiene la intruccion open con el path del archivo csv proporcionado\n",
    "        # se graga ruta relativa del archivo\n",
    "        datos= open(\"data\\\\malformed_dataset.csv\",'r')\n",
    "        \n",
    "        # se crea la variable reader\n",
    "        reader = csv.reader(datos)\n",
    "\n",
    "        #para que no apllique cambios al encabezado\n",
    "        header = next(reader)\n",
    "         \n",
    "\n",
    "         # a los valores de la fila 0 los reemplaza por NaN\n",
    "        for fila in reader:\n",
    "            if (fila [0])==\"\":\n",
    "                fila [0] = 'NaN'\n",
    "            else:\n",
    "                fila [0]==fila [0]\n",
    "            \n",
    "\n",
    "            # if para evaluar si la fila 1 es numerico\n",
    "            if not fila[1].isnumeric():\n",
    "                fila[1] =\"NaN\"\n",
    "            else:\n",
    "                fila[1] = int(fila[1])\n",
    "\n",
    "            \n",
    "            ##remplazo de la fila 2 a flotante\n",
    "            if not fila[2].replace('.', '', 1).isdigit(): # si no se puede convertir a flotante\n",
    "                   fila[2] = \"NaN\"\n",
    "            else:\n",
    "                  fila[2] = float(fila[2])\n",
    "\n",
    "\n",
    "            ##remplazo de la fila 3 a flotante\n",
    "            if not fila[3].replace('.', '', 1).isdigit(): # si no se puede convertir a flotante\n",
    "                   fila[3] = \"NaN\"\n",
    "            else:\n",
    "                  fila[3] = float(fila[3])\n",
    "            \n",
    "            ##remplazo de la fila 3 a flotante\n",
    "            if not fila[4].replace('.', '', 1).isdigit(): # si no se puede convertir a flotante\n",
    "                   fila[4] = \"NaN\"\n",
    "            else:\n",
    "                  fila[4] = float(fila[4])\n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "            print (fila)    \n",
    "                \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def infer_dtypes(self, reader) -> dict:\n",
    "        # CODE GOES HERE\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return \n",
    "\n",
    "    def describe(self) -> dict:\n",
    "        # CODE GOES HERE \n",
    "\n",
    "        return\n",
    "\n",
    "ejemplo = Data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a60ba2ceecc33b4bd5a6ba271705321769a23e291572eda34e6d023e570ba350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
